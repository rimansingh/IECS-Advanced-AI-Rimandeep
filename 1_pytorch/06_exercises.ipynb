{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 3D Tensor dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we create a custom dataset class named Random3DTensorDataset that extends PyTorch's Dataset. This dataset generates random 3D tensors and corresponding random binary labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor batch shape: torch.Size([10, 3, 32, 32])\n",
      "Labels: tensor([0, 0, 0, 1, 1, 1, 0, 1, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class Random3DTensorDataset(Dataset):\n",
    "    def __init__(self, num_samples: int, tensor_shape: tuple = (3, 32, 32)):\n",
    "        self.num_samples = num_samples\n",
    "        self.tensor_shape = tensor_shape\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        tensor = torch.rand(*self.tensor_shape)\n",
    "        label = torch.randint(0, 2, ())\n",
    "        return tensor, label\n",
    "\n",
    "# Create dataset and DataLoader\n",
    "dataset = Random3DTensorDataset(num_samples=100, tensor_shape=(3, 32, 32))\n",
    "dataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Fetch one batch\n",
    "for batch_tensors, batch_labels in dataloader:\n",
    "    print(\"Tensor batch shape:\", batch_tensors.shape)\n",
    "    print(\"Labels:\", batch_labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datastreamers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 3, 32, 32])\n",
      "tensor([0, 1, 0, 0, 1, 0, 0, 1, 0, 1])\n",
      "\n",
      "\n",
      "torch.Size([10, 3, 32, 32])\n",
      "tensor([0, 1, 0, 0, 1, 1, 1, 1, 0, 1])\n",
      "\n",
      "\n",
      "torch.Size([10, 3, 32, 32])\n",
      "tensor([1, 0, 0, 0, 1, 1, 0, 1, 1, 1])\n",
      "\n",
      "\n",
      "torch.Size([10, 3, 32, 32])\n",
      "tensor([1, 0, 0, 0, 1, 0, 1, 1, 1, 0])\n",
      "\n",
      "\n",
      "torch.Size([10, 3, 32, 32])\n",
      "tensor([1, 0, 0, 0, 0, 1, 0, 0, 1, 1])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "\n",
    "class BaseDatastreamer:\n",
    "    def __init__(self, dataset, batchsize, preprocessor=None, max_batches=None):\n",
    "        self.dataset = dataset\n",
    "        self.batchsize = batchsize\n",
    "        self.preprocessor = preprocessor\n",
    "        self.max_batches = max_batches\n",
    "\n",
    "    def stream(self):\n",
    "        batch_count = 0\n",
    "        while True:\n",
    "            if self.max_batches and batch_count >= self.max_batches:\n",
    "                break\n",
    "\n",
    "            indices = list(range(len(self.dataset)))\n",
    "            random.shuffle(indices)\n",
    "            for i in range(0, len(indices), self.batchsize):\n",
    "                batch_indices = indices[i: i + self.batchsize]\n",
    "                batch = [self.dataset[idx] for idx in batch_indices]\n",
    "                \n",
    "                batch_tensors, batch_labels = zip(*batch)  \n",
    "                \n",
    "                batch_tensors = torch.stack(batch_tensors)  \n",
    "                batch_labels = torch.tensor(batch_labels)\n",
    "                \n",
    "                if self.preprocessor:\n",
    "                    batch_tensors = self.preprocessor(batch_tensors)\n",
    "\n",
    "                yield batch_tensors, batch_labels\n",
    "                batch_count += 1\n",
    "\n",
    "\n",
    "def batch_processor(batch_tensors):\n",
    "    return batch_tensors * 2\n",
    "\n",
    "# Create dataset\n",
    "dataset = Random3DTensorDataset(num_samples=100, tensor_shape=(3, 32, 32))\n",
    "\n",
    "# Create a datastreamer with a limit of 5 batches\n",
    "streamer = BaseDatastreamer(dataset=dataset, batchsize=10, preprocessor=batch_processor, max_batches=5)\n",
    "\n",
    "data_gen = streamer.stream()\n",
    "\n",
    "for _ in range(5):\n",
    "    batch_tensors, batch_labels = next(data_gen)\n",
    "    print(batch_tensors.shape)\n",
    "    print(batch_labels)\n",
    "    print(\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Tune the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mads_datasets import DatasetFactoryProvider, DatasetType\n",
    "\n",
    "from mltrainer.preprocessors import BasePreprocessor\n",
    "from mltrainer import imagemodels, Trainer, TrainerSettings, ReportTypes, metrics\n",
    "\n",
    "import torch.optim as optim\n",
    "import gin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParsedConfigFileIncludesAndImports(filename='model.gin', imports=['gin.torch.external_configurables'], includes=[])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gin.parse_config_file(\"model.gin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-17 08:58:39.813\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m121\u001b[0m - \u001b[1mFolder already exists at /Users/rimansingh/.cache/mads_datasets/fashionmnist\u001b[0m\n",
      "\u001b[32m2025-02-17 08:58:39.823\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmads_datasets.base\u001b[0m:\u001b[36mdownload_data\u001b[0m:\u001b[36m124\u001b[0m - \u001b[1mFile already exists at /Users/rimansingh/.cache/mads_datasets/fashionmnist/fashionmnist.pt\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "preprocessor = BasePreprocessor()\n",
    "fashionfactory = DatasetFactoryProvider.create_factory(DatasetType.FASHION)\n",
    "streamers = fashionfactory.create_datastreamer(batchsize=64, preprocessor=preprocessor)\n",
    "train = streamers[\"train\"]\n",
    "valid = streamers[\"valid\"]\n",
    "trainstreamer = train.stream()\n",
    "validstreamer = valid.stream()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import gin.torch.external_configurables\n",
      "\n",
      "# Parameters for NeuralNetwork:\n",
      "# ==============================================================================\n",
      "NeuralNetwork.num_classes = 10\n",
      "NeuralNetwork.units1 = 512\n",
      "NeuralNetwork.units2 = 16\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(gin.config_str())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = metrics.Accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-17 08:36:36.106\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.settings\u001b[0m:\u001b[36mcheck_path\u001b[0m:\u001b[36m61\u001b[0m - \u001b[1mCreated logdir /Users/rimansingh/University/sem-3/Adv-AI/IECS-Advanced-AI-Rimandeep/1_pytorch/models\u001b[0m\n",
      "\u001b[32m2025-02-17 08:36:36.163\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-083636\u001b[0m\n",
      "\u001b[32m2025-02-17 08:36:36.166\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "  0%|\u001b[38;2;30;71;6m          \u001b[0m| 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:11<00:00, 79.29it/s]\n",
      "\u001b[32m2025-02-17 08:36:48.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5111 test 0.4178 metric ['0.8433']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:15<00:00, 60.35it/s]\n",
      "\u001b[32m2025-02-17 08:37:05.261\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3682 test 0.3834 metric ['0.8585']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:13<00:00, 72.00it/s]\n",
      "\u001b[32m2025-02-17 08:37:19.507\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3275 test 0.3545 metric ['0.8733']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:18<00:00, 50.37it/s]\n",
      "\u001b[32m2025-02-17 08:37:39.251\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3036 test 0.3488 metric ['0.8743']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:19<00:00, 48.40it/s]\n",
      "\u001b[32m2025-02-17 08:38:00.151\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.2878 test 0.3471 metric ['0.8749']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [01:23<00:00, 16.79s/it]\n",
      "\u001b[32m2025-02-17 08:38:00.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-083800\u001b[0m\n",
      "\u001b[32m2025-02-17 08:38:00.201\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:11<00:00, 79.76it/s]\n",
      "\u001b[32m2025-02-17 08:38:12.849\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5219 test 0.4177 metric ['0.8517']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:18<00:00, 51.93it/s]\n",
      "\u001b[32m2025-02-17 08:38:32.405\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3701 test 0.3711 metric ['0.8668']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:15<00:00, 59.46it/s]\n",
      "\u001b[32m2025-02-17 08:38:49.063\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3346 test 0.3676 metric ['0.8649']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:14<00:00, 64.78it/s]\n",
      "\u001b[32m2025-02-17 08:39:04.416\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3069 test 0.3529 metric ['0.8726']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:15<00:00, 60.55it/s]\n",
      "\u001b[32m2025-02-17 08:39:20.767\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.2891 test 0.3463 metric ['0.8747']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [01:20<00:00, 16.10s/it]\n",
      "\u001b[32m2025-02-17 08:39:20.779\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-083920\u001b[0m\n",
      "\u001b[32m2025-02-17 08:39:20.787\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:13<00:00, 67.34it/s] \n",
      "\u001b[32m2025-02-17 08:39:35.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5459 test 0.4252 metric ['0.8481']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 86.82it/s]\n",
      "\u001b[32m2025-02-17 08:39:47.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3801 test 0.3966 metric ['0.8551']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:12<00:00, 76.94it/s]\n",
      "\u001b[32m2025-02-17 08:40:00.970\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3395 test 0.3691 metric ['0.8645']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 87.45it/s]\n",
      "\u001b[32m2025-02-17 08:40:12.536\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3162 test 0.3745 metric ['0.8644']\u001b[0m\n",
      "\u001b[32m2025-02-17 08:40:12.538\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3691, current loss 0.3745.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 88.31it/s]\n",
      "\u001b[32m2025-02-17 08:40:24.195\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.2977 test 0.3489 metric ['0.8730']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [01:03<00:00, 12.67s/it]\n",
      "\u001b[32m2025-02-17 08:40:24.206\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-084024\u001b[0m\n",
      "\u001b[32m2025-02-17 08:40:24.217\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 100.98it/s]\n",
      "\u001b[32m2025-02-17 08:40:34.419\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5275 test 0.4727 metric ['0.8305']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:11<00:00, 80.81it/s]\n",
      "\u001b[32m2025-02-17 08:40:46.836\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3746 test 0.3823 metric ['0.8625']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 98.72it/s] \n",
      "\u001b[32m2025-02-17 08:40:57.053\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3371 test 0.3752 metric ['0.8607']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 120.23it/s]\n",
      "\u001b[32m2025-02-17 08:41:05.426\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3168 test 0.3474 metric ['0.8741']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 126.68it/s]\n",
      "\u001b[32m2025-02-17 08:41:13.552\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.2963 test 0.3480 metric ['0.8782']\u001b[0m\n",
      "\u001b[32m2025-02-17 08:41:13.553\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3474, current loss 0.3480.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:49<00:00,  9.87s/it]\n",
      "\u001b[32m2025-02-17 08:41:13.573\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-084113\u001b[0m\n",
      "\u001b[32m2025-02-17 08:41:13.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 144.26it/s]\n",
      "\u001b[32m2025-02-17 08:41:20.565\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5492 test 0.4583 metric ['0.8343']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:07<00:00, 123.42it/s]\n",
      "\u001b[32m2025-02-17 08:41:28.773\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3817 test 0.3834 metric ['0.8606']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 98.97it/s]\n",
      "\u001b[32m2025-02-17 08:41:39.144\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3434 test 0.3822 metric ['0.8627']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:15<00:00, 60.48it/s]\n",
      "\u001b[32m2025-02-17 08:41:56.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3201 test 0.3620 metric ['0.8736']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:14<00:00, 66.24it/s] \n",
      "\u001b[32m2025-02-17 08:42:11.281\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.3025 test 0.3784 metric ['0.8659']\u001b[0m\n",
      "\u001b[32m2025-02-17 08:42:11.283\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3620, current loss 0.3784.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:57<00:00, 11.54s/it]\n",
      "\u001b[32m2025-02-17 08:42:11.299\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-084211\u001b[0m\n",
      "\u001b[32m2025-02-17 08:42:11.302\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:06<00:00, 149.38it/s]\n",
      "\u001b[32m2025-02-17 08:42:18.136\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5675 test 0.4532 metric ['0.8397']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 99.47it/s]\n",
      "\u001b[32m2025-02-17 08:42:28.275\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3956 test 0.4082 metric ['0.8532']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 92.78it/s]\n",
      "\u001b[32m2025-02-17 08:42:39.160\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3522 test 0.3810 metric ['0.8637']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 89.64it/s]\n",
      "\u001b[32m2025-02-17 08:42:50.698\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3268 test 0.3768 metric ['0.8601']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 86.40it/s]\n",
      "\u001b[32m2025-02-17 08:43:02.433\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.3079 test 0.3585 metric ['0.8692']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:51<00:00, 10.22s/it]\n",
      "\u001b[32m2025-02-17 08:43:02.444\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-084302\u001b[0m\n",
      "\u001b[32m2025-02-17 08:43:02.452\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 94.08it/s]\n",
      "\u001b[32m2025-02-17 08:43:13.215\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5531 test 0.4415 metric ['0.8413']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 98.38it/s] \n",
      "\u001b[32m2025-02-17 08:43:23.424\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.3872 test 0.4216 metric ['0.8480']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 95.17it/s]\n",
      "\u001b[32m2025-02-17 08:43:33.949\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3506 test 0.3822 metric ['0.8648']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 98.70it/s] \n",
      "\u001b[32m2025-02-17 08:43:44.181\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3255 test 0.3585 metric ['0.8714']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 88.98it/s]\n",
      "\u001b[32m2025-02-17 08:43:55.500\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.3082 test 0.3650 metric ['0.8688']\u001b[0m\n",
      "\u001b[32m2025-02-17 08:43:55.501\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3585, current loss 0.3650.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:53<00:00, 10.60s/it]\n",
      "\u001b[32m2025-02-17 08:43:55.518\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-084355\u001b[0m\n",
      "\u001b[32m2025-02-17 08:43:55.524\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:10<00:00, 92.35it/s] \n",
      "\u001b[32m2025-02-17 08:44:06.391\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5751 test 0.4532 metric ['0.8386']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 96.23it/s]\n",
      "\u001b[32m2025-02-17 08:44:17.146\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.4026 test 0.3954 metric ['0.8575']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:13<00:00, 71.38it/s] \n",
      "\u001b[32m2025-02-17 08:44:31.656\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3569 test 0.3854 metric ['0.8598']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:15<00:00, 59.54it/s]\n",
      "\u001b[32m2025-02-17 08:44:48.612\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3349 test 0.4122 metric ['0.8546']\u001b[0m\n",
      "\u001b[32m2025-02-17 08:44:48.614\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3854, current loss 0.4122.Counter 1/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:13<00:00, 68.98it/s]\n",
      "\u001b[32m2025-02-17 08:45:03.216\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.3157 test 0.3903 metric ['0.8575']\u001b[0m\n",
      "\u001b[32m2025-02-17 08:45:03.218\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__call__\u001b[0m:\u001b[36m234\u001b[0m - \u001b[1mbest loss: 0.3854, current loss 0.3903.Counter 2/10.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [01:07<00:00, 13.53s/it]\n",
      "\u001b[32m2025-02-17 08:45:03.230\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mdir_add_timestamp\u001b[0m:\u001b[36m29\u001b[0m - \u001b[1mLogging to models/20250217-084503\u001b[0m\n",
      "\u001b[32m2025-02-17 08:45:03.270\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m72\u001b[0m - \u001b[1mFound earlystop_kwargs in settings.Set to None if you dont want earlystopping.\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:12<00:00, 75.12it/s] \n",
      "\u001b[32m2025-02-17 08:45:16.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 0 train 0.5918 test 0.4602 metric ['0.8372']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:09<00:00, 102.16it/s]\n",
      "\u001b[32m2025-02-17 08:45:26.344\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 1 train 0.4139 test 0.4391 metric ['0.8434']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 104.51it/s]\n",
      "\u001b[32m2025-02-17 08:45:36.033\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 2 train 0.3747 test 0.3924 metric ['0.8605']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 105.93it/s]\n",
      "\u001b[32m2025-02-17 08:45:45.583\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 3 train 0.3517 test 0.3787 metric ['0.8633']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 937/937 [00:08<00:00, 109.13it/s]\n",
      "\u001b[32m2025-02-17 08:45:54.851\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mmltrainer.trainer\u001b[0m:\u001b[36mreport\u001b[0m:\u001b[36m191\u001b[0m - \u001b[1mEpoch 4 train 0.3323 test 0.3690 metric ['0.8699']\u001b[0m\n",
      "100%|\u001b[38;2;30;71;6m██████████\u001b[0m| 5/5 [00:51<00:00, 10.32s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "gin.parse_config_file(\"model.gin\")\n",
    "\n",
    "units = [256, 128, 64]\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "settings = TrainerSettings(\n",
    "    epochs=5,\n",
    "    metrics=[accuracy],\n",
    "    logdir=\"models\",\n",
    "    train_steps=len(train),\n",
    "    valid_steps=len(valid),\n",
    "    reporttypes=[ReportTypes.TENSORBOARD, ReportTypes.GIN],\n",
    ")\n",
    "\n",
    "for unit1 in units:\n",
    "    for unit2 in units:\n",
    "        gin.bind_parameter(\"NeuralNetwork.units1\", unit1)\n",
    "        gin.bind_parameter(\"NeuralNetwork.units2\", unit2)\n",
    "\n",
    "        model = imagemodels.NeuralNetwork()\n",
    "        trainer = Trainer(\n",
    "            model=model,\n",
    "            settings=settings,\n",
    "            loss_fn=loss_fn,\n",
    "            optimizer=optim.Adam,\n",
    "            traindataloader=trainstreamer,\n",
    "            validdataloader=validstreamer,\n",
    "            scheduler=optim.lr_scheduler.ReduceLROnPlateau\n",
    "        )\n",
    "        trainer.loop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updated Experiment Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
